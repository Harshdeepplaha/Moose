# import pandas as pd
# from model import initialize_llm  # Import model initialization

# # Initialize LLM instance
# llm = initialize_llm()

# def generate_chat_response(user_question, df):
#     """
#     Generates a response from the LLM based on a user's question and the transaction data.
    
#     Parameters:
#     - user_question (str): The question or query posed by the user.
#     - df (DataFrame): The user's categorized transaction data.
    
#     Returns:
#     - str: The response generated by the LLM.
#     """
#     # Format the data context for the LLM prompt
#     data_context = df.to_dict(orient="records")  # Convert DataFrame to list of dictionaries
#     prompt = f"{user_question} based on the following transaction data: {data_context}"
    
#     # Generate response from LLM
#     try:
#         response = llm.invoke(prompt)
#     except Exception as e:
#         print(f"Error generating chat response: {e}")
#         response = "I'm sorry, I encountered an error while processing your request."
    
#     return response

# def ask_about_category_spending(category, df):
#     """
#     Generates a response regarding spending within a specific category.
    
#     Parameters:
#     - category (str): The category for which the user wants spending information.
#     - df (DataFrame): The user's categorized transaction data.
    
#     Returns:
#     - str: The LLM-generated response about spending in the specified category.
#     """
#     # Filter transactions for the specified category
#     category_data = df[df['Category'] == category]
#     category_total = category_data['Amount (EUR)'].sum()
    
#     # Prepare a summary of category spending
#     prompt = (
#         f"My total spending in the category '{category}' is €{category_total:.2f}. "
#         f"Here is the breakdown of transactions in this category: {category_data.to_dict(orient='records')}."
#         "Can you summarize or provide insights into this spending?"
#     )
    
#     # Generate response from LLM
#     try:
#         response = llm.invoke(prompt)
#     except Exception as e:
#         print(f"Error generating category spending response: {e}")
#         response = f"I'm sorry, I encountered an error while processing your request for category '{category}'."
    
#     return response

# def ask_about_saving_goals(target_saving, current_saving):
#     """
#     Generates a response based on the user's saving goals.
    
#     Parameters:
#     - target_saving (float): The user's target savings amount.
#     - current_saving (float): The user's current savings amount.
    
#     Returns:
#     - str: The LLM's advice or feedback on the user's saving goal progress.
#     """
#     prompt = (
#         f"My target saving goal is €{target_saving:.2f}, and my current savings amount is €{current_saving:.2f}. "
#         "Can you suggest strategies or insights to help me reach my goal?"
#     )
    
#     # Generate response from LLM
#     try:
#         response = llm.invoke(prompt)
#     except Exception as e:
#         print(f"Error generating saving goals response: {e}")
#         response = "I'm sorry, I encountered an error while processing your request about saving goals."
    
#     return response



import pandas as pd
from model import initialize_llm  # Import model initialization

# Initialize LLM instance
llm = initialize_llm()

def generate_chat_response(user_question, df):
    """
    Generates a response from the LLM based on a user's question and the transaction data.
    
    Parameters:
    - user_question (str): The question or query posed by the user.
    - df (DataFrame): The user's categorized transaction data.
    
    Returns:
    - str: The response generated by the LLM.
    """
    # Summarize data context for prompt (limited sample)
    data_summary = df[['Date', 'Name / Description', 'Expense/Income', 'Amount (EUR)', 'Category']].head(5).to_string(index=False)
    prompt = (
        f"You are an AI financial assistant. The user has the following question: '{user_question}'. "
        f"Here is a sample of the user's transaction data:\n\n{data_summary}\n\n"
        "Answer the question based on this data, providing insights if possible."
    )
    
    # Generate response from LLM
    try:
        response = llm.invoke(prompt)
        print("LLM Response:", response)  # Debug: print the LLM response to ensure it’s generated
    except Exception as e:
        print(f"Error generating chat response: {e}")
        response = "I'm sorry, I encountered an error while processing your request."
    
    return response

